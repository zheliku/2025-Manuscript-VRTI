Figure 1. (a)​​ illustrates a user performing gesture interaction while wearing a VR headset. In the top portion, the user's right hand adopts a grasping posture (highlighted by a red dashed bounding box) with the caption "Something Missing?" indicating the absence of haptic feedback. The bottom portion shows the same gesture interacting with a cube, captioned "It's Real Touch!", where contrasting emoticons (frowning face above vs. smiling face below) emphasize the experiential difference before and after haptic feedback integration. ​(b)​​ shows the VRTI interaction pipeline: The main scene shows a user (wearing a white maple-leaf-patterned T-shirt) simultaneously manipulating both virtual and real objects in VR. The VR headset points to the right-side "Virtual Interaction" panel, which displays three virtual hand actions (grasping, pressing, and pinching), representing visual feedback directed to the user. The user's right hand grasps a physical block, connected via arrow to the "Real Interaction" panel that demonstrates three corresponding physical hand actions, providing realistic haptic feedback. The bidirectional flow of visual and tactile information is explicitly synchronized, as emphasized by the "Real-time synchronization" label.

Figure 2. ​Figure​​ demonstrates an experimental scenario for verifying momentum conservation. The central area features three interactive VR twins. The background displays a visualization interface showing three real-time charts: v-t (velocity vs. time), P-t (momentum vs. time), and Ek-t (kinetic energy vs. time), along with corresponding physics formulas. The left panel allows users to toggle between different charts, while the right panel provides adjustable parameters for object mass and spring stiffness, enabling dynamic manipulation of experimental conditions.

Figure 3. Figure​​ presents detailed structural designs of three VR Twins (Puller, Button, and Knob). The Puller consists of a spring-loaded block mechanism, the Button integrates a switch mounted on a base, and the Knob features a rotatable handle attached to a stationary base. All three Reality-Interface Objects (RIOs) were fabricated as Virtual-Interface Objects (VIOs) via 3D printing.

Figure 4. Figure​​ illustrates the complete interaction flow of three VR Twins. The Puller interaction involves grasping the block, pulling it backward, and releasing; the Button interaction requires pressing the switch followed by hand retraction; and the Knob interaction entails pinching the handle and rotating it about its central axis.

Figure 5. The left half shows the physical arrangement of RIOs, where the Puller, Button, and Knob are outlined in green, yellow, and blue wireframes respectively, forming a triangular distribution. The right half illustrates the corresponding virtual counterparts (VIOs) in identical color-coded wireframes, with their relative positions precisely matching those in the physical space.

Figure 6. ​(a) A right hand forms a cup-shaped grip around a white cube, demonstrating the correct grasping relationship in physical interaction. (b) In the virtual environment, while the hand model maintains the same posture, tracking inaccuracies cause positional displacement, resulting in partial penetration of the hand model into the virtual cube. (c) The virtual hand retains the same misaligned position as in (b), but the cube exhibits significant positional and angular deviation from (a) due to conventional physics-based collision response.

Figure 7. (a) Puller employs a semi-closed grasping gesture, where fingers form a cup-shaped contour conforming to the cubic block's surface; (b) Button uses an extended-palm pressing gesture, with the palm fully contacting the switch's top surface; (c) Knob adopts a pinching-rotation gesture, where clustered fingers grasp the handle's head portion for rotational control.

Figure 8. (a) A spherical bounding volume centered at the palm, with its boundary demarcated by green dashed lines; (b) A sphere-tree model constructed from hand joints (knuckles, palm center, etc.), where spherical colliders are positioned at each joint location (boundaries shown in green dashed lines) and adjacent joints are connected by red solid lines.

Figure 9. (a) In the virtual environment, a right hand grasps a yellow cube, with hand-tracking positions indicated by blue squares (hand joints) and thick red lines (adjacent joint connections). Notably, the thumb and index finger penetrate the yellow cube's interior. The white hand silhouette represents the user-perceived virtual hand that appears properly aligned with the cube's surface.(b) The same right hand is shown fully extended and pressing downward on the yellow cube, where spherical colliders at each joint effectively prevent penetration through physics-based constraints.

Figure 10. The user's real hands engage in two parallel processes: (1) Real Interaction with Reality-Interface Objects (RIOs) of VR Twins to obtain realistic haptic feedback, and (2) Motion Capture via Quest 3 cameras, where hand-tracking data is transmitted to Unity for virtual hand generation. VIO achieves synchronization by processing RIO sensor data while performing real-time collision detection with scene objects. The user's virtual hands enable dual interaction capabilities: Manipulating control panels to adjust scene object properties, Directly controlling RIO through VIO. The Unity engine renders the synthesized scene to the Quest 3 display, ultimately presenting the visual output to the user's eyes.

Figure 11. (a) Instruction Phase: Participants stand in front of computer workstations while observing instructional videos; (b) Operation Phase: Participants wearing VR headsets conduct momentum conservation experiments, with researchers providing on-site guidance to ensure protocol compliance.

Figure 12. Presents comparative results between experimental (VRTI) and control (GI) groups across pre-test performance and user experience dimensions. (a) Pre-test Scores. Fundamental physics concepts: VRTI (M=4.5) vs. GI (M=4.28). Momentum knowledge: VRTI (M=6.56) vs. GI (M=6.59). Total scores: VRTI (M=11.1) vs. GI (M=10.9). No significant differences were observed (all p>0.05). (b) User Experience Metrics (kernel density estimates and boxplots). ​​Cognitive load​​: No significant difference (Median=2.75 for both) ​Learning motivation​​: VRTI (Median=4.8) significantly outperformed GI (Median=4) (p<0.001) Immersion: VRTI (Median=4) showed significantly higher immersion than GI (Median=3.29) (p<0.001).

Figure 13. Detailed comparison between the experimental group (VRTI) and control group (GI) across three dimensions: cognitive load, learning motivation, and immersion. For each sub-dimension, VRTI demonstrated statistically significant superiority over GI.

Figure 14. Boxplot comparing performance scores between VRTI (experimental) and GI (control) groups across four metrics: Momentum Concept, Experimental Understanding, Comprehensive Application, and Total Score. Key observations: (1) Momentum Concept: VRTI (median=0) compares GI (median=0) with no significance. (2) Experimental Understanding: VRTI (median=0) exceeds GI (median=0) with *-level significance. (3) Total Score: VRTI (median=1) significantly compares GI (median=1​) with no significance. (4) Comprehensive Application: VRTI (median=3.5) exceeds GI (median=2) with **-level significance.